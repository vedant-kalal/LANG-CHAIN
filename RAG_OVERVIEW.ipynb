{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee73cdbb",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG): A Comprehensive Overview\n",
    "\n",
    "## 1. What is RAG?\n",
    "Retrieval-Augmented Generation (RAG) is a technique that enhances the accuracy and reliability of generative AI models by fetching facts from an external knowledge base. instead of relying solely on the vast but static training data of a Large Language Model (LLM), RAG allows the model to reference precise, up-to-date, or proprietary information before generating a response.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Features & Benefits\n",
    "*   **Accuracy & Reliability**: Reduces \"hallucinations\" by grounding answers in retrieved facts.\n",
    "*   **Up-to-Date Information**: access recent data not included in the LLM's training set without re-training.\n",
    "*   **Contextual Awareness**: Can specific domain knowledge (legal, medical, internal company docs).\n",
    "*   **Transparency**: Can cite sources from the retrieved documents, building user trust.\n",
    "*   **Cost-Effective**: Cheaper than fine-tuning an entire model for new data.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The RAG Workflow (Steps & Examples)\n",
    "\n",
    "The RAG process is generally divided into two main phases: **Indexing (Preparation)** and **Retrieval & Generation (Execution)**.\n",
    "\n",
    "### Phase 1: Indexing (Data Preparation)\n",
    "\n",
    "#### Step 1: Document Loading\n",
    "**Description**: Importing raw data from various sources (PDFs, Websites, Databases).\n",
    "**Example**: Loading a company policy PDF.\n",
    "`Loader = PyPDFLoader(\"policy.pdf\")`\n",
    "\n",
    "#### Step 2: Text Splitting (Chunking)\n",
    "**Description**: Breaking large documents into smaller, manageable chunks. This is crucial because LLMs have context windows, and we want to retrieve only the relevant parts.\n",
    "**Example**: Splitting the policy into 500-character chunks with a 50-character overlap.\n",
    "`Splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)`\n",
    "\n",
    "#### Step 3: Embedding\n",
    "**Description**: Converting text chunks into vector representations (arrays of numbers) that capture semantic meaning.\n",
    "**Example**: The sentence \"The refund policy is 30 days\" is converted to `[0.12, -0.45, 0.88, ...]`.\n",
    "`Embeddings = OpenAIEmbeddings()`\n",
    "\n",
    "#### Step 4: Vector Storage\n",
    "**Description**: Storing the vectors and their original text in a Vector Database (like Chroma, FAISS, Pinecone) for fast similarity search.\n",
    "**Example**: Saving the chunks into a local ChromaDB.\n",
    "`VectorStore = Chroma.from_documents(documents=chunks, embedding=Embeddings)`\n",
    "\n",
    "### Phase 2: Execution (Retrieval & Generation)\n",
    "\n",
    "#### Step 5: User Query & Embedding\n",
    "**Description**: The user asks a question. This query is also converted into a vector (embedding) using the same model as Step 3.\n",
    "**Input**: \"What is the refund window?\"\n",
    "**Vector**: `[0.10, -0.42, 0.85, ...]` (similar to the indexed chunk).\n",
    "\n",
    "#### Step 6: Retrieval\n",
    "**Description**: The system searches the Vector Database for the top *k* chunks that are mathematically most similar (closest in vector space) to the query embedding.\n",
    "**Result**: Retrieves the chunk: *\"Customers may request a full refund within 30 days of purchase...\"*\n",
    "\n",
    "#### Step 7: Augmentation & Generation\n",
    "**Description**: The retrieved context + the original user query are combined into a prompt for the LLM.\n",
    "**Prompt Construction**: \n",
    "> \"Answer the question based strictly on the context below:\n",
    "> Context: Customers may request a full refund within 30 days of purchase...\n",
    "> Question: What is the refund window?\"\n",
    "\n",
    "**LLM Output**: \"The refund window is 30 days from the date of purchase.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Types of RAG\n",
    "\n",
    "### 1. Naive RAG (The Basic Pipeline)\n",
    "The standard \"Retrieve-Read-Generate\" process described above.\n",
    "*   **Pros**: Simple to implement.\n",
    "*   **Cons**: Prone to precision issues (retrieving irrelevant chunks) or recall issues (missing the answer).\n",
    "\n",
    "### 2. Advanced RAG\n",
    "Introduces optimizations to improve performance.\n",
    "*   **Pre-Retrieval Optimization**: improving the data indexing (better chunking, metadata addition) or query optimization (query rewriting, expansion) before searching.\n",
    "    *   *Example*: Rewriting \"It's broken\" to \"The device screen is cracked\" for better matching.\n",
    "*   **Post-Retrieval Optimization**: Re-ranking the retrieved documents to ensure the most relevant ones are at the top before sending to the LLM.\n",
    "    *   *Example*: Using a Cross-Encoder to score relevance.\n",
    "\n",
    "### 3. Modular RAG\n",
    "A flexible architecture where modules can be added or swapped.\n",
    "*   **Search Module**: Can include web search APIs (SerpAPI) alongside vector stores.\n",
    "*   **Memory Module**: Storing conversation history to handle follow-up questions.\n",
    "*   **Routing**: Deciding which tool or database to query based on the question complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary Example\n",
    "**Scenario**: an AI for a car manual.\n",
    "1.  **Ingest**: You load the PDF manual.\n",
    "2.  **User asks**: \"How do I change the oil?\"\n",
    "3.  **Retrieve**: System finds page 42 (\"Oil Change Procedures\").\n",
    "4.  **Generate**: LLM reads page 42 and summarizes: \"First, locate the drain plug under the engine...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8569308",
   "metadata": {},
   "source": [
    "## RAG Architecture Diagram\n",
    "\n",
    "![RAG Architecture Diagram](images/rag_diagram.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
