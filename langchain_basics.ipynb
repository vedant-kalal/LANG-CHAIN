{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a17a6c65",
            "metadata": {},
            "source": [
                "# LangChain: A Comprehensive Guide\n",
                "\n",
                "**LangChain** is a powerful framework designed to simplify the development of applications powered by Large Language Models (LLMs). It acts as a bridge between the raw computational power of LLMs and the real-world data and interactions required to build useful applications.\n",
                "\n",
                "Below is a detailed breakdown of the **6 Core Concepts** of LangChain. If you understand these, you have the foundation to build almost any LLM application.\n",
                "\n",
                "For complete description, please refer to the [LangChain Documentation](https://learnwith.campusx.in/products/LangChain-Playlist-Notes-68679680c2a2f25bbcc8efb6).\n",
                "---\n",
                "\n",
                "\n",
                "## 1. Models (The Brain)\n",
                "\n",
                "The heart of any LangChain application is the language model. LangChain distinguishes between two main types of models, even though they are often backed by the same underlying technology (like GPT-4).\n",
                "\n",
                "### A. Large Language Models (LLMs)\n",
                "*   **Concept**: Pure text-in, text-out engines.\n",
                "*   **Input/Output**: Takes a text string as input and returns a text string.\n",
                "*   **Use Case**: Text completion, simple generation.\n",
                "*   **Example Interaction**: \n",
                "    *   *Input*: \"Tell me a joke.\"\n",
                "    *   *Output*: \"Why did the chicken cross the road...\"\n",
                "*   **LangChain Interface**: `LLM` class.\n",
                "\n",
                "### B. Chat Models\n",
                "*   **Concept**: Models tuned for conversation, understanding roles.\n",
                "*   **Input/Output**: Takes a list of *Messages* as input and returns a single *Message*.\n",
                "*   **Structure**: Messages are usually categorized as:\n",
                "    *   `SystemMessage`: Instructions for the AI's behavior (e.g., \"You are a helpful assistant\").\n",
                "    *   `HumanMessage`: The user's input.\n",
                "    *   `AIMessage`: The model's response.\n",
                "*   **Use Case**: Chatbots, conversational agents, complex instruction following.\n",
                "*   **LangChain Interface**: `ChatModel` class.\n",
                "\n",
                "---\n",
                "\n",
                "## 2. Prompts (The Instructions)\n",
                "\n",
                "Raw models are sensitive to how you phrase requests. **Prompts** are the interface between the user and the model. LangChain provides tools to construct, manage, and optimize these inputs.\n",
                "\n",
                "### A. Prompt Templates\n",
                "Hard-coding prompts is inflexible. Templates allow you to create dynamic prompts using variables.\n",
                "*   **Example**: Instead of writing \"Translate 'Hello' to Spanish\", you create a template: `\"Translate '{text}' to {language}\"`.\n",
                "*   **Benefit**: Reusability, cleaner code, and separation of logic from text.\n",
                "\n",
                "### B. Output Parsers\n",
                "Models output text, but often you need structured data (like JSON, a list, or a specific date format). Output Parsers take the raw text response and transform it into a usable format.\n",
                "*   **Example**: You ask for a list of 5 cities. The parser ensures you get a Python list `['Paris', 'London', ...]` instead of a string \"Here are the cities: 1. Paris...\"\n",
                "\n",
                "---\n",
                "\n",
                "## 3. Chains (The Workflow)\n",
                "\n",
                "A single call to an LLM is often not enough for complex tasks. **Chains** allow you to link multiple operations together in a deterministic sequence.\n",
                "\n",
                "### A. LLMChain\n",
                "The most fundamental chain. It combines a **PromptTemplate**, a **Model**, and an optional **Output Parser**.\n",
                "*   *Flow*: User Input -> Prompt Template -> Formatted Prompt -> LLM -> Response.\n",
                "\n",
                "### B. Sequential Chains\n",
                "Connects multiple chains where the output of one becomes the input of the next.\n",
                "*   **SimpleSequentialChain**: Single input/output stepping. (A -> B -> C)\n",
                "*   **SequentialChain**: Multiple inputs/outputs, allowing for complex workflows (e.g., Chain 1 summarizes a text, Chain 2 translates the summary, Chain 3 writes a critique of the translation).\n",
                "\n",
                "---\n",
                "\n",
                "## 4. Memory (The Context)\n",
                "\n",
                "LLMs are **stateless** by default. If you ask \"Hi, I'm Bob\" and then \"What's my name?\", the model won't know unless you send the context again. **Memory** stores the history of interactions.\n",
                "\n",
                "### Common Memory Types:\n",
                "*   **ConversationBufferMemory**: Stores the raw text of the conversation. Good for short contexts.\n",
                "*   **ConversationBufferWindowMemory**: Keeps a list of the last *K* interactions. Good for keeping the prompt size manageable.\n",
                "*   **ConversationSummaryMemory**: Uses an LLM to summarize the conversation so far and stores that summary. Excellent for long-running conversations where keeping every word is too expensive.\n",
                "\n",
                "---\n",
                "\n",
                "## 5. Indexes / RAG (The Knowledge Base)\n",
                "\n",
                "Standard LLMs are trained on public data up to a certain cutoff date. They don't know your private data (company PDFs, emails, databases). **Retrieval Augmented Generation (RAG)** solves this by feeding relevant data to the model at runtime.\n",
                "\n",
                "### The RAG Pipeline:\n",
                "1.  **Document Loaders**: Utilities to load data from sources (Text files, PDFs, GitHub, S3, Web pages).\n",
                "2.  **Text Splitters**: LLMs have a \"context window\" (limit on text size). Splitters break large documents into smaller, semantically meaningful chunks.\n",
                "3.  **Embeddings**: A model that converts text into a vector (a list of numbers). Similar text has similar vectors.\n",
                "4.  **Vector Stores**: Databases optimized to store and search these vectors (e.g., Pinecone, Chroma, FAISS).\n",
                "5.  **Retrievers**: The algorithm that searches the Vector Store for chunks relevant to the user's query and passes them to the LLM as context.\n",
                "\n",
                "---\n",
                "\n",
                "## 6. Agents (The Decision Makers)\n",
                "\n",
                "In Chains, the sequence of actions is hardcoded. **Agents** use an LLM as a reasoning engine to determine *what* actions to take and in *what order*.\n",
                "\n",
                "### Components:\n",
                "*   **Tools**: Functions the agent can call (e.g., \"Google Search\", \"Calculator\", \"Python REPL\").\n",
                "*   **Toolkit**: A collection of tools for a specific task (e.g., \"SQLDatabaseToolkit\").\n",
                "*   **Agent Executor**: The runtime loop:\n",
                "    1.  Agent receives user input.\n",
                "    2.  Agent \"thinks\" and decides to use a Tool.\n",
                "    3.  Tool executes and returns an \"Observation\".\n",
                "    4.  Agent receives the observation and decides the next step or the final answer.\n",
                "\n",
                "### Why Agents are Powerful\n",
                "Unlike chains, agents can handle ambiguity. If you ask \"Who is the girlfriend of the actor who played Spiderman in 2002?\", an agent can figure out it needs to: 1. Search for the 2002 Spiderman actor, 2. Search for his girlfriend, 3. Return the answer."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
