{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YISgZibC0Pxs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fSCXxQpEzSnX"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HaQH5oowzwE-"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    Document(page_content = \"langchain helps developers build llm application easily\"),\n",
        "    Document(page_content = \"Chroma is a vector database optimized for LLM-based search\"),\n",
        "    Document(page_content = \"Embeddings convert text into high-dimensional vectors\"),\n",
        "    Document(page_content = \"Hugging Face provides embedding models\"),\n",
        "    Document(page_content = \"OpenAI provides embedding models\"),\n",
        "    Document(page_content = \"FAISS is a library for efficient similarity search\"),\n",
        "    Document(page_content = \"MMR stands for Maximum Marginal Relevance\"),\n",
        "    Document(page_content = \"langchain makes easy to work with llms\"),\n",
        "    Document(page_content = \"Chroma is a vector database\"),\n",
        "    Document(page_content = \"langchain provides many different functions for llm aplications\"),\n",
        "    Document(page_content = \"MMR helps you get diverse results when doing similarity search\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXgqfl6nzw5-"
      },
      "outputs": [],
      "source": [
        "vector_store = FAISS.from_documents(\n",
        "    documents = documents,\n",
        "    embedding= HuggingFaceEmbeddings()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Xp5PCldAzyQj"
      },
      "outputs": [],
      "source": [
        "retriver1 = vector_store.as_retriever(\n",
        "    search_type = \"mmr\", # this enables MMR\n",
        "    search_kwargs = {\"k\":3 , \"lambda_mult\":1, \"fetch_k\": 10}) # k = desired number of final results, lambda_mult = relevance-diversity balance, fetch_k = number of documents to fetch before MMR.\n",
        "# lambda_mult (0-1), 0: very diverse answers, 1: more relevant answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XUOnidYC2bDZ"
      },
      "outputs": [],
      "source": [
        "retriver2 = vector_store.as_retriever(\n",
        "    search_type = \"mmr\", # this enables MMR\n",
        "    search_kwargs = {\"k\":3 , \"lambda_mult\":0, \"fetch_k\": 10}) # k = desired number of final results, lambda_mult = relevance-diversity balance, fetch_k = number of documents to fetch before MMR.\n",
        "# lambda_mult (0-1), 0: very diverse answers, 1: more relevant answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pZHJdjpr0IUJ"
      },
      "outputs": [],
      "source": [
        "query = \"what is langchain?\"\n",
        "results1 = retriver1.invoke(query)\n",
        "results2 = retriver2.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e48w-YC40J90",
        "outputId": "495f8391-948b-428d-a436-3a8e2ff843ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ----\n",
            "langchain provides many different functions for llm aplications\n",
            "\n",
            "--- Result 2 ----\n",
            "langchain makes easy to work with llms\n",
            "\n",
            "--- Result 3 ----\n",
            "langchain helps developers build llm application easily\n"
          ]
        }
      ],
      "source": [
        "for i,doc in enumerate(results1): # when lambda_mult :1 , less diverse\n",
        "  print(f\"\\n--- Result {i+1} ----\")\n",
        "  print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ptGRIgt2iH-",
        "outputId": "cb989dfd-fdea-4ab4-b094-0cb5f2801f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ----\n",
            "langchain provides many different functions for llm aplications\n",
            "\n",
            "--- Result 2 ----\n",
            "Hugging Face provides embedding models\n",
            "\n",
            "--- Result 3 ----\n",
            "MMR stands for Maximum Marginal Relevance\n"
          ]
        }
      ],
      "source": [
        "for i,doc in enumerate(results2): # when lambda_mult :0 , more diverse\n",
        "  print(f\"\\n--- Result {i+1} ----\")\n",
        "  print(doc.page_content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
