{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T67ZuVZpkycQ"
      },
      "source": [
        "## VECTOR STORE RETRIEVER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "BhJ2Xd5JksaX"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "YVeLW2uplO3j"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    Document(page_content = \"langchain helps developers build llm application easily\"),\n",
        "    Document(page_content = \"Chroma is a vector database optimized for LLM-based search\"),\n",
        "    Document(page_content = \"Embeddings convert text into high-dimensional vectors\"),\n",
        "    Document(page_content = \"OpenAI provides powerful embedding models\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtSkOM53lWZJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents = documents,\n",
        "    embedding= HuggingFaceEmbeddings(),\n",
        "    persist_directory=\"./chroma_db_\",\n",
        "    collection_name=\"langchain_features\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "dI1DbAuqqm2k"
      },
      "outputs": [],
      "source": [
        "# convert vectorstore into a retriver\n",
        "retriver = vector_store.as_retriever(search_kwarfs = {\"k\":1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "gEmNNgIFq-Me"
      },
      "outputs": [],
      "source": [
        "query = \"what is chroma used for ?\"\n",
        "results = retriver.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRFLPJPNrL84",
        "outputId": "d752cc82-4de7-457c-aca3-0ea29ce47a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ----\n",
            "Chroma is a vector database optimized for LLM-based search\n",
            "\n",
            "--- Result 2 ----\n",
            "Chroma is a vector database optimized for LLM-based search\n",
            "\n",
            "--- Result 3 ----\n",
            "langchain helps developers build llm application easily\n",
            "\n",
            "--- Result 4 ----\n",
            "langchain helps developers build llm application easily\n"
          ]
        }
      ],
      "source": [
        "for i,doc in enumerate(results):\n",
        "  print(f\"\\n--- Result {i+1} ----\")\n",
        "  print(doc.page_content)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
